{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyscan6003/CE5021_AY2526_group1/blob/pionicolafiano_ce5021/Assignment2_student_PioNicolaFiano_sem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0od727REF6w"
      },
      "source": [
        "#Transfer Learning with Med MNIST.\n",
        "\n",
        "In this assignment you will use transfer learning to train a model of your choice on a sub-dataset from the [MedMNIST datasets](https://medmnist.com/). ![](https://github.com/tonyscan6003/etivities/blob/main/medmnist.JPG?raw=true)\n",
        "\n",
        "* The [MedMNIST package](https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb) allows the data to be imported directly as a pytorch dataset.\n",
        "\n",
        "* You may select any of the datasets using Multi-class/binary classification. The goal is to acheve accuracy levels comparable to the benchmark results shown on the medmnist site. Dataloading for pytorch is setup in the notebook, you wil need to modify the code slightly depending on your dataset of choice.\n",
        "\n",
        "* Some datasets use black and white images, so you will need to [at least concatenate](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) the input image channels (to 3 channels) for compatibility with the models pre-trained on imageNet.\n",
        "\n",
        "* Some of the MedMNIST datasets don't contain too much data so Data augmentation may be essentila essential to avoid overfitting. In pytorch data augmentation is performed using the [transforms.v2](https://pytorch.org/vision/main/transforms.html) modules.\n",
        "\n",
        "* In this notebook: You will need to import a model, and perform training. Tranfer Learning for computer vision is detailed [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "\n",
        "* [Tensorboard can be imported](https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html) to display results.\n",
        "\n",
        "* Please only include one example of transfer learning in the submitted notebook. Making sure training curves/results are clearly visible. If you have trained additional transfer learning models (i.e. that were less successful) please add this as a table or report at the end of the notebook and/or in your final forum post.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02jbObyemiYx"
      },
      "source": [
        "# 1. Install & Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ALnzL_qv0J9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340dfe83-191b-40fc-b52c-15b173fee1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (12.0.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.8.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.23.0+cpu)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2025.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->medmnist) (3.0.3)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, medmnist\n",
            "Successfully installed fire-0.7.1 medmnist-3.0.2\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install medmnist\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DEt0NrOfEBpA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import ToTensor, Pad, Grayscale\n",
        "import torchvision.transforms.v2 as v2 # Corrected import for v2\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S65n7FefE5T7"
      },
      "source": [
        "## 2. Setup & Import Dataset\n",
        "The [MedMNSIT](https://medmnist.com/) package (imported above) makes available several medical datasets available to access.\n",
        "\n",
        "You can change the `data_flag` variable (dataset names are all lower case letters) to the dataset of your choice (Take care to note the parameters e.g. number of input channels below that will affect your model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "caFqq_2T5uaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488691b3-8407-42e6-822a-da0f69f19d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of Machine Learning Task =  multi-class\n",
            "Number of Input Data Channels =  3\n",
            "Number of Classes =  9\n",
            "The batch size for this dataset will be =  64\n"
          ]
        }
      ],
      "source": [
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "\n",
        "#data_flag = 'bloodmnist'\n",
        "data_flag = 'pathmnist'\n",
        "download = True\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "print('Type of Machine Learning Task = ',task)\n",
        "print('Number of Input Data Channels = ',n_channels)\n",
        "print('Number of Classes = ',n_classes)\n",
        "print('The batch size for this dataset will be = ',BATCH_SIZE)\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEcCkv14yJzj"
      },
      "source": [
        "Transforms:\n",
        "You can update the functions below with appropriate transforms for your particular use case.\n",
        "\n",
        "* As well as being suitable for data augmention for image classification, the transforms.v2 package of torchvision extends transforms for object detection and segmentation tasks. An illustration of the transforms is shown [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py).\n",
        "* Normalisation based on ImageNet parameters is included already. This should be used with all models pre-trained on ImageNet  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = v2.Compose([\n",
        "    #v2.Grayscale(num_output_channels=3),          # PathMNIST → RGB\n",
        "    v2.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.2),\n",
        "    v2.RandomRotation(15),\n",
        "    v2.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    ToTensor(),\n",
        "    v2.Normalize([0.485, 0.456, 0.406],\n",
        "                 [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transforms = v2.Compose([\n",
        "    #v2.Grayscale(num_output_channels=3),\n",
        "    v2.Resize(256),\n",
        "    v2.CenterCrop(224),\n",
        "\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    ToTensor(),\n",
        "\n",
        "    v2.Normalize([0.485, 0.456, 0.406],\n",
        "                 [0.229, 0.224, 0.225]),\n",
        "])\n"
      ],
      "metadata": {
        "id": "ubK3vCaiFIFN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daS_Z6OayI3y"
      },
      "source": [
        "Setup your Medmnist dataset.\n",
        "* The convolution part of pre-trained networks (such as resnet) are compatable with any size input image (A convolution layer slides a kernel over the height and width of an image. A 3×3 kernel does the same operation whether the input is 64×64, 224×224, or 512×800. The number of weights (parameters) in the kernel stays the same no matter how large the input is. But, even though the kernel parameters don’t change, the output feature map does change.\n",
        "). However they were trained on 224 x 224 size images, with early layers finding small scale features and deeper layers finding large scale features.\n",
        "* For this transfer learning application to medical data, **the gap between the original ImageNet domain and the medical images is wide**. Therefore the size/scale of the input images is less important, however in general we would expect better performance with the larger input images (as they contain more features at different scales).\n",
        "* You can add the `size=224` parameter to the dataset object calls, to load full size images. Only do this once you are confident in your training methodology (or if the dataset is small), as training with full size images will take longer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mSh1TTskS_H"
      },
      "source": [
        "ResNet (and similar CNNs) are pre-trained on ImageNet. Characteristic:\n",
        "* natural images\n",
        "* RGB\n",
        "* many textures\n",
        "* objects, animals, outdoor scenes\n",
        "\n",
        "Medical images, depending on modality, are:\n",
        "* grayscale or non-RGB distributions\n",
        "* highly homogeneous textures\n",
        "* anatomical structures instead of natural patterns\n",
        "* much more subtle local features\n",
        "* often very different contrast (low contrast), noise (e.g. unwanted random variation in pixel values), and scale properties (see Literature Review section below)\n",
        "\n",
        "*This domain gap is huge!*\n",
        "\n",
        "Because of this gap, the low-level ImageNet filters (edges, corners, color blobs) still transfer, but the higher-level object-specific filters transfer poorly.\n",
        "\n",
        "→ *This makes the exact input resolution less crucial than we might expect, compared to natural images!*\n",
        "\n",
        "If the semantics change drastically, resizing from 224 → 384 → 512 does not fundamentally “restore” ImageNet-style features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi1WSCszkS_H"
      },
      "source": [
        "**Semantics** = the meaning of the content in the image.\n",
        "\n",
        "Examples of semantic content:\n",
        "* A dog → fur, legs, head\n",
        "* A car → wheels, metal surfaces\n",
        "* A lung X-ray → ribs, soft tissue, air spaces\n",
        "* An MRI → brain structures, gray matter, lesions\n",
        "*Semantics = what the image is, not how big it is.*, therefore resizing cannot change the semantics\n",
        "\n",
        "ImageNet models learned high-level concepts such as:\n",
        "* fur texture\n",
        "* animal eyes\n",
        "* wheels\n",
        "* buildings\n",
        "* grass\n",
        "* sky\n",
        "\n",
        "A medical image contains none of these.\n",
        "\n",
        "So even if we feed a larger image (384/512), the pre-trained model:\n",
        "* still sees completely different patterns\n",
        "* still activates filters designed for natural images\n",
        "* still has mismatched high-level feature detectors\n",
        "\n",
        "Resizing improves resolution, but does not change the underlying domain mismatch.\n",
        "\n",
        "To summarize: *Increasing image resolution helps preserve detail, but it cannot make a medical image behave like a natural ImageNet image, because the underlying content (semantics) is entirely different*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IMQwoRRkS_H"
      },
      "source": [
        "**Literature Review**\n",
        "1. Contrast differences\n",
        "\n",
        "**Contrast** = differences in brightness between regions.\n",
        "\n",
        "*ImageNet / natural images*\n",
        "\n",
        "High contrast (bright skies, dark shadows)\n",
        "\n",
        "Color variation (RGB)\n",
        "\n",
        "Objects clearly separated from backgrounds\n",
        "\n",
        "Example: A dog on the grass → clear boundaries, strong textures, color differences.\n",
        "\n",
        "*Medical images*\n",
        "\n",
        "Often low contrast\n",
        "\n",
        "X-rays: many shades of grey with subtle differences\n",
        "\n",
        "Ultrasound: soft transitions, lots of haze\n",
        "\n",
        "MRI: tissues differ only slightly in intensity\n",
        "\n",
        "No color channels → grayscale or custom modality (CT HU units, MRI T1/T2 weighting)\n",
        "\n",
        "--> *Interpretation is harder because abnormalities may differ by only 1–5% pixel intensity.*\n",
        "\n",
        "2. Noise differences\n",
        "\n",
        "**Noise** = unwanted random variation in pixel values.\n",
        "\n",
        "*Natural images*\n",
        "\n",
        "Usually clean\n",
        "\n",
        "Noise mainly from low light or bad camera quality\n",
        "\n",
        "Cameras produce RGB sensor noise, which has specific patterns\n",
        "\n",
        "*Medical images*\n",
        "\n",
        "Each modality introduces its own type of noise:\n",
        "\n",
        "Modality\tTypical noise\n",
        "X-ray: photon noise, scattering artifacts\n",
        "MRI\tGaussian: Rician noise\n",
        "Ultrasound: speckle noise (grainy texture)\n",
        "CT scan: Poisson noise, reconstruction artifacts\n",
        "\n",
        "These noise patterns look nothing like the noise ImageNet models are used to learning.\n",
        "\n",
        "--> *CNNs trained on ImageNet may misinterpret medical noise as texture or edges.*\n",
        "\n",
        "3. Scale differences\n",
        "\n",
        "**Scale** = the size of meaningful features in the image.\n",
        "\n",
        "*ImageNet images*\n",
        "\n",
        "Objects appear at many scales\n",
        "\n",
        "a dog can fill the whole frame\n",
        "\n",
        "a bird can be small\n",
        "\n",
        "Backgrounds vary widely\n",
        "\n",
        "Scale is inconsistent → good for CNN generalization\n",
        "\n",
        "*Medical images*\n",
        "\n",
        "Organs and anatomical structures have consistent, fixed scales\n",
        "\n",
        "the size of a lung\n",
        "\n",
        "the shape of a vertebra\n",
        "\n",
        "the thickness of skin layers\n",
        "\n",
        "Abnormalities may be extremely small\n",
        "\n",
        "microcalcifications\n",
        "\n",
        "tiny lesions\n",
        "\n",
        "hairline fractures\n",
        "\n",
        "--> *CNNs must be sensitive to fine details, whereas ImageNet models aren’t optimized for such tiny-scale patterns.*\n",
        "\n",
        "**Why these differences matter for transfer learning**\n",
        "\n",
        "ResNet50 learned filters on *ImageNet* that expect:\n",
        "* strong edges\n",
        "* vibrant colors\n",
        "* high contrast\n",
        "* natural textures (grass, fur, wood, sky)\n",
        "* objects that vary in size\n",
        "\n",
        "In medical imaging, it encounters:\n",
        "* weak edges\n",
        "* grayscale\n",
        "* subtle shape changes\n",
        "* faint lesions\n",
        "* fixed anatomical scales\n",
        "* modality-specific noise\n",
        "\n",
        "This is why the domain shift is huge, pre-trained features can still help, but not as strongly as in natural-image tasks.\n",
        "\n",
        "To summarize: *Medical images differ from ImageNet images in contrast (how bright/dark features appear), noise (the type of grain or artifacts present), and scale (the typical size of structures and abnormalities), which makes transfer learning less straightforward.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dggcL7P4kS_H"
      },
      "source": [
        "1. MedMNIST datasets do not store large images by default.\n",
        "Most are very small, e.g.:\n",
        "* PathMNIST → 28×28\n",
        "* OrganMNIST → 28×28\n",
        "* ChestMNIST → 28×28\n",
        "* RetinaMNIST → 28×28 (grayscale)\n",
        "\n",
        "When I write: train_dataset = DataClass(split='train', size=224, ...)\n",
        "\n",
        "I tell MedMNIST: “Load the images AND resize them to 224×224 for me.”\n",
        "\n",
        "This is done *before transforms*, so the images are now:\n",
        "* bigger\n",
        "* higher-resolution (synthetically)\n",
        "* compatible with ImageNet models, which expect 224×224 input\n",
        "\n",
        "2. Why use 224×224 at all?\n",
        "\n",
        "Because ImageNet models (ResNet, EfficientNet, etc.) were trained at 224×224.\n",
        "\n",
        "If my medical images are kept at 28×28:\n",
        "* they contain very little detail\n",
        "* ResNet must massively downsample them (stride 2 convs)\n",
        "* the model sees almost no spatial information\n",
        "* performance will likely be poor\n",
        "\n",
        "So resizing to 224×224:\n",
        "* makes the network behave as intended\n",
        "* preserves more detail (even though upsampled)\n",
        "* produces larger feature maps at deeper layers\n",
        "* improves transfer-learning accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bPuGQ1JucvV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGcCuvXjE7TI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa22bdf-1cc1-4e90-edae-7345b6031936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 514M/12.6G [06:51<2:08:22, 1.57MB/s]"
          ]
        }
      ],
      "source": [
        "download = True # Changed to True to ensure data download if not present\n",
        "# Download training data from open datasets.\n",
        "train_dataset = DataClass(split='train', transform=train_transforms, download=download, size=224, mmap_mode='r')\n",
        "test_dataset = DataClass(split='test', transform=test_transforms, download=download, size=224, mmap_mode='r')\n",
        "val_dataset = DataClass(split='val', transform=test_transforms, download=download, size=224, mmap_mode='r')\n",
        "\n",
        "#mmap_mode='r' --> Load the dataset from disk using memory-mapped mode, read-only.\n",
        "#size=224 --> It tells MedMNIST: \"Resize every image to 224×224 pixels.\" Why? Because ImageNet pre-trained models expect 224×224 images.\n",
        "#If omitted, many MedMNIST datasets default to 28×28\n",
        "#download=download If download=True, MedMNIST will: download the dataset from the official repository, store it locally, avoid re-downloading if already cached"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH_kTIFYIoTA"
      },
      "source": [
        "The `Dataset` is passed as an argument to `DataLoader`. This wraps an\n",
        "iterable over the dataset, and supports automatic batching, sampling,\n",
        "shuffling and multiprocess data loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX-snmo0Iv3v"
      },
      "outputs": [],
      "source": [
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr4srFL0ZDU7"
      },
      "source": [
        "Plot some example augmented images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofqjq8b4ZIJk"
      },
      "outputs": [],
      "source": [
        "# Output next batch from dataloader\n",
        "dataiter = iter(train_dataloader)\n",
        "image_batch, labels_batch = next(dataiter)\n",
        "\n",
        "# Use matplotlib to plot a sample of images\n",
        "\n",
        "i=0\n",
        "n_plots = 12 # number of plots\n",
        "f, axarr = plt.subplots(1,n_plots,figsize=(20,10))\n",
        "\n",
        "for image in image_batch[0:n_plots,:,:,:]:\n",
        "  disp_image =  torch.permute(image,(2,1,0)).numpy() # return image to cpu for display and permute to channels last\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  disp_image = std * disp_image + mean\n",
        "  disp_image = np.clip(disp_image, 0, 1)\n",
        "  axarr[i].imshow(disp_image[:,:,:])\n",
        "  axarr[i].axis(\"off\")\n",
        "  axarr[i].set_title(labels_batch[i].numpy(),fontsize='small')\n",
        "  i = i+1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIIjqCm9IbxS"
      },
      "source": [
        "# 3. Define Transfer Learning model\n",
        "Pytorch has an inbuilt [models package](https://pytorch.org/vision/stable/models.html) that allows loading of popular models with pre-trained weights.\n",
        "\n",
        "* We want to add an additional classifier stage (to the output of the network). How to setup the [model is detailed here](https://discuss.pytorch.org/t/load-only-a-part-of-the-network-with-pretrained-weights/88397/2).\n",
        "* This additional classifier may just be a single layer or a cascade of fully connected layers with dropout.\n",
        "* Note that the number of parameters in the convolutional part of the model will be same no what the input size is set to. However the output feature map size will vary with input image size (small for small image, large for large image). This means the number of parameters in the additional classifier will change depending on input image size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yno_8ZPUkS_I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# Imports the neural-network module and binds it to the name nn.\n",
        "# nn contains layers (nn.Linear, nn.Conv2d), loss functions,\n",
        "# nn.Module base class, nn.Identity, etc.\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "# Specifically imports the resnet50 constructor and the ResNet50_Weights helper\n",
        "# enum/class provided by torchvision for selecting pretrained weight bundles\n",
        "# (this pattern was introduced in newer torchvision versions).\n",
        "# resnet50 is a function that returns a ResNet model configured with the\n",
        "# ResNet-50 architecture.\n",
        "\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "\n",
        "# Chooses the default weights configuration (usually a set of pretrained weights\n",
        "# such as ImageNet) via the ResNet50_Weights helper.\n",
        "# weights is typically an enum-like object holding metadata about normalization,\n",
        "# URL, transforms, and the weights tensor. Passing this to resnet50(...) tells\n",
        "# torchvision to load those pretrained weights.\n",
        "\n",
        "backbone = resnet50(weights=weights)\n",
        "# Builds a ResNet-50 model and loads the chosen pretrained weights into it.\n",
        "# backbone is an instance of torchvision.models.resnet.ResNet configured for\n",
        "# ResNet-50 (contains layers like conv1, layer1..layer4, avgpool, and fc).\n",
        "# By default the final fully connected layer backbone.fc matches the number of\n",
        "# classes from the pretrained weights (e.g., 1000 for ImageNet) unless num_classes is passed to resnet50(...).\n",
        "# Note: the model parameters returned have param.requires_grad = True by\n",
        "# default — they will be trainable unless you freeze them later.\n",
        "\n",
        "# Let's define a new class named MyResNet50 that subclasses nn.Module,\n",
        "# the base class for all PyTorch models. This wrapper will reuse the pretrained\n",
        "# ResNet backbone but swap out the classification head.\n",
        "class MyResNet50(nn.Module):\n",
        "    def __init__(self, backbone_model, num_classes=9, hidden_dim=512):\n",
        "        super(MyResNet50, self).__init__()\n",
        "        # keep all layers except the final FC\n",
        "        self.backbone = backbone_model\n",
        "        in_features = self.backbone.fc.in_features   # should be 2048 for resnet50\n",
        "\n",
        "        # replace the backbone fc with identity so forward(backbone) produces features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "        # new classifier head\n",
        "        #self.classifier = nn.Sequential(\n",
        "         #   nn.Linear(in_features, hidden_dim),\n",
        "          #  nn.ReLU(inplace=True),\n",
        "           # nn.Dropout(0.2),\n",
        "           # nn.Linear(hidden_dim, num_classes)\n",
        "        #)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)      # shape (N, 2048)\n",
        "        out = self.classifier(features)  # shape (N, num_classes)\n",
        "        return out\n",
        "\n",
        "model = MyResNet50(backbone_model=backbone, num_classes=9)\n",
        "\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ1W5n704brZ"
      },
      "outputs": [],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlaqCj_xkS_I"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHA32e7IkS_I"
      },
      "source": [
        "*resize_size=[232]*\n",
        "\n",
        "The pipeline will resize the input image so that the shorter side is 232 pixels (preserving aspect ratio).\n",
        "\n",
        "(Behaviour: when size is an int in Resize, torchvision typically resizes the smaller edge to that value, keeping aspect ratio.)\n",
        "\n",
        "This step increases/decreases the raw image size while keeping aspect ratio so the subsequent crop has the expected field of view.\n",
        "\n",
        "*crop_size=[224]*\n",
        "\n",
        "After resizing, the pipeline applies a center crop of size 224×224.\n",
        "\n",
        "The model was trained on center-cropped 224×224 images, so this replicates training preprocessing.\n",
        "\n",
        "Why resize to 232 then crop to 224?\n",
        "\n",
        "Resizing the short side to a slightly larger number (232) and then center-cropping 224 is a standard ImageNet convention: it reduces the effect of small translations while maintaining a consistent crop area that the model saw in training.\n",
        "\n",
        "*mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]*\n",
        "\n",
        "These are the per-channel ImageNet normalization statistics (RGB order).\n",
        "\n",
        "After converting image pixels to floating values in [0,1], the pipeline subtracts the mean and divides by the std for each channel:\n",
        "\n",
        "x_norm[c] = (x[c] - mean[c]) / std[c]\n",
        "\n",
        "This centers and scales each channel to have similar distributions the model expects.\n",
        "\n",
        "*interpolation=InterpolationMode.BILINEAR*\n",
        "\n",
        "When resizing, bilinear interpolation is used to compute new pixel values (smooth, good general-purpose choice).\n",
        "\n",
        "Interpolation affects how details change when resizing; bilinear is commonly used for photos and model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69un-V55kS_I"
      },
      "outputs": [],
      "source": [
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "# PyTorch modules (especially CNNs like ResNet-50) behave differently during training vs inference.\n",
        "# eval() switches the model to inference behaviour.\n",
        "\n",
        "# BatchNorm → stops updating running stats, uses stored ones\n",
        "# Dropout → turns off randomness\n",
        "# Linear, Conv, ReLU, Pooling → unaffected\n",
        "# The custom classifier → behaves normally (unless includes dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3_9Lw9zOtNJ"
      },
      "source": [
        "# 4. Optimisation & Training Loop\n",
        "\n",
        "Define a training loop that prints the loss from the training and validation set at least every epoch.\n",
        "* You may choose to make the validation loss calculations more frequent so you can ensure training is progressing satisfactorily (especially with larger datasets).\n",
        "\n",
        "* You can use tensorboard to visualise the [loss curves](https://pytorch.org/docs/stable/tensorboard.html))\n",
        "\n",
        "* For multi-class classification classification problem we will use the  [CrossEntropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). In this pytorch function, the input is the un-normalised logit value.\n",
        "\n",
        "* You may  have to use [torch.squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) to reduce the dimensionality of the label tensor before passing it to the loss function (this due to how the dataset is configured and the loss will only accept 0D or 1D inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r21Zjf3wx4Nt"
      },
      "source": [
        "Initialise Tensorboard (use of tensorboard in colab notebooks is [detailed here](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb)). A [Pytorch tutorial](https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html) shows how to setup the training and validation loop with pytorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "nzrywUe1Bxb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd9d6fJIx7w1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Loss Function ---\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Optimizer: only classifier parameters ---\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.classifier.parameters(),   # new classifier head layers only\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4\n",
        ")"
      ],
      "metadata": {
        "id": "zLJKjlTfg90c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Early Stopping Class\n",
        "# -------------------------\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ],
      "metadata": {
        "id": "JtK20JB6OEPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iter = 10\n",
        "n_epochs = 10\n",
        "train_size = len(train_dataloader.dataset)\n",
        "test_size = len(test_dataloader.dataset)\n",
        "num_test_batches = len(test_dataloader)\n",
        "\n",
        "# --- Move model to device ---\n",
        "model.to(device)\n",
        "\n",
        "# --- Optimizer (Classifier only) ---\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.classifier.parameters(),\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=n_epochs,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Initialize Early Stopping\n",
        "# -------------------------\n",
        "early_stopper = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "best_vloss = np.inf\n",
        "\n",
        "# --- Training Loop ---\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Reset patience at the beginning of each epoch\n",
        "    early_stopper.counter = 0\n",
        "    early_stopper.early_stop = False\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        labels = torch.squeeze(labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += (outputs.argmax(1) == labels).float().sum().item()\n",
        "\n",
        "        # ------- Validation Step -------\n",
        "        if i % n_iter == n_iter - 1:\n",
        "            print(f'Epoch {epoch}  Batch {i+1}')\n",
        "\n",
        "            model.eval()\n",
        "            running_vloss = 0.0\n",
        "            running_vacc = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for vinputs, vlabels in test_dataloader:\n",
        "                    vinputs, vlabels = vinputs.to(device), vlabels.to(device)\n",
        "                    vlabels = torch.squeeze(vlabels)\n",
        "\n",
        "                    voutputs = model(vinputs)\n",
        "                    vloss = loss_fn(voutputs, vlabels)\n",
        "\n",
        "                    running_vloss += vloss.item()\n",
        "                    running_vacc += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            avg_loss = running_loss / n_iter\n",
        "            avg_acc = 100 * running_acc / (n_iter * BATCH_SIZE)\n",
        "\n",
        "            avg_vloss = running_vloss / num_test_batches\n",
        "            avg_vacc = 100 * running_vacc / test_size\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "\n",
        "            print(f\"Training Error:\\n Accuracy: {avg_acc:>0.1f}%, Loss: {avg_loss:.6f}\")\n",
        "            print(f\"Valid Error:\\n Accuracy: {avg_vacc:>0.1f}%, Loss: {avg_vloss:.6f}\\n\")\n",
        "\n",
        "            writer.add_scalars('Loss',\n",
        "                               {'Training Loss': avg_loss, 'Validation Loss': avg_vloss},\n",
        "                               epoch * len(train_dataloader) + i)\n",
        "            writer.add_scalars('Accuracy',\n",
        "                               {'Training Acc': avg_acc, 'Validation Acc': avg_vacc},\n",
        "                               epoch * len(train_dataloader) + i)\n",
        "\n",
        "            # -------------------------\n",
        "            # Early Stopping Check\n",
        "            # -------------------------\n",
        "            if avg_vloss < best_vloss:\n",
        "                best_vloss = avg_vloss\n",
        "                torch.save(model.state_dict(), \"best_model.pth\")\n",
        "                print(\"Saved new best model\\n\")\n",
        "\n",
        "            early_stopper(avg_vloss)\n",
        "            if early_stopper.early_stop:\n",
        "                print(\"Early stopping triggered!\")\n",
        "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "                print(\"Restored best model.\")\n",
        "                writer.flush()\n",
        "                break\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Finished Training\")\n",
        "writer.flush()\n"
      ],
      "metadata": {
        "id": "E7te-IRwOd1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs --bind_all"
      ],
      "metadata": {
        "id": "CmgFtJHVmhDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3KEd881eG-0"
      },
      "source": [
        "# 5. Evaluate Model\n",
        "You will need to setup an evaluation loop for the model to assess it's performance on the test dataset.\n",
        "\n",
        "You may also obtain a classification report after final evaluation of the test dataset with the model using the code below. A confusion matrix can also be obtained and we will plot a few example images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"background\",\n",
        "    \"adipose\",\n",
        "    \"debris\",\n",
        "    \"lymphocytes\",\n",
        "    \"epithelial\",\n",
        "    \"stromal\",\n",
        "    \"muscle\",\n",
        "    \"mucus\",\n",
        "    \"necrosis\",\n",
        "]\n",
        "\n",
        "store_predictions = []\n",
        "store_labels = []\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "    # basic training loop\n",
        "    input_batch, label_batch = data\n",
        "    input_batch = input_batch.to(device)\n",
        "\n",
        "    pred_logit = model(input_batch)\n",
        "    predictions = torch.argmax(pred_logit,1) # reduce along output dimension\n",
        "    predictions_np = predictions.to(\"cpu\").numpy()\n",
        "    label_batch_np = label_batch.numpy()\n",
        "    if i<num_test_batches-1:\n",
        "      store_predictions.append(predictions_np)\n",
        "      store_labels.append(label_batch)\n",
        "\n",
        "y_pred = np.squeeze(np.reshape(store_predictions,(1,(num_test_batches-1)*batch_size)))\n",
        "y_true = np.squeeze(np.reshape(store_labels,(1,(num_test_batches-1)*batch_size)))"
      ],
      "metadata": {
        "id": "V0QgXHy5kny0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jbe4FkwkEjB"
      },
      "outputs": [],
      "source": [
        "#https://scikit-learn.org/0.16/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "plot_confusion_matrix(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN2167c_kKRj"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output next batch from dataloader\n",
        "dataiter = iter(test_dataloader)\n",
        "image_batch, labels_batch = next(dataiter)\n",
        "\n",
        "# Pass batch through model\n",
        "model.eval()\n",
        "image_batch = image_batch.to(device)\n",
        "pred_logit = model(image_batch)\n",
        "\n",
        "\n",
        "# Use matplotlib to plot a sample of images\n",
        "import matplotlib.pyplot as plt\n",
        "i=0\n",
        "n_plots = 12 # number of plots\n",
        "f, axarr = plt.subplots(1,n_plots,figsize=(20,10))\n",
        "\n",
        "\n",
        "for image in image_batch[0:n_plots,:,:,:]:\n",
        "  disp_image =  torch.permute(image.to('cpu'),(2,1,0)).numpy() # return image to cpu for display and permute to channels last\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  disp_image = std * disp_image + mean\n",
        "  disp_image = np.clip(disp_image, 0, 1)\n",
        "  axarr[i].imshow(disp_image[:,:,:])\n",
        "  axarr[i].axis(\"off\")\n",
        "  predicted, actual = classes[pred_logit[i,:].argmax(0)], classes[labels_batch[i]]\n",
        "  color = 'black' if predicted == actual else 'red'\n",
        "  axarr[i].set_title(predicted,fontsize='small', color=color)\n",
        "  i = i+1"
      ],
      "metadata": {
        "id": "KiF-5fchlctL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkZQ0yQS4JTL"
      },
      "source": [
        " # Comments about your Approach"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}