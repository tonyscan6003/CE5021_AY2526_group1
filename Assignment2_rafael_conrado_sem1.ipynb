{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning with Med MNIST.\n",
        "\n",
        "In this assignment you will use transfer learning to train a model of your choice on a sub-dataset from the [MedMNIST datasets](https://medmnist.com/). ![](https://github.com/tonyscan6003/etivities/blob/main/medmnist.JPG?raw=true)\n",
        "\n",
        "* The [MedMNIST package](https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb) allows the data to be imported directly as a pytorch dataset.\n",
        "\n",
        "* You may select any of the datasets using Multi-class/binary classification. The goal is to acheve accuracy levels comparable to the benchmark results shown on the medmnist site. Dataloading for pytorch is setup in the notebook, you wil need to modify the code slightly depending on your dataset of choice.\n",
        "\n",
        "* Some datasets use black and white images, so you will need to [at least concatenate](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) the input image channels (to 3 channels) for compatibility with the models pre-trained on imageNet.\n",
        "\n",
        "* Some of the MedMNIST datasets don't contain too much data so Data augmentation may be essentila essential to avoid overfitting. In pytorch data augmentation is performed using the [transforms.v2](https://pytorch.org/vision/main/transforms.html) modules.\n",
        "\n",
        "* In this notebook: You will need to import a model, and perform training. Tranfer Learning for computer vision is detailed [here](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
        "\n",
        "* [Tensorboard can be imported](https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html) to display results.\n",
        "\n",
        "* Please only include one example of transfer learning in the submitted notebook. Making sure training curves/results are clearly visible. If you have trained additional transfer learning models (i.e. that were less successful) please add this as a table or report at the end of the notebook and/or in your final forum post.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D0od727REF6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install & Import Packages"
      ],
      "metadata": {
        "id": "02jbObyemiYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install medmnist\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "ALnzL_qv0J9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1aff396-46dc-4ac1-b12a-2a120366fd4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (11.3.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.24.0+cu126)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->medmnist) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->medmnist) (3.0.3)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fire, medmnist\n",
            "Successfully installed fire-0.7.1 medmnist-3.0.2\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DEt0NrOfEBpA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, models\n",
        "from torchvision.transforms import ToTensor, v2, Pad, Grayscale\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup & Import Dataset\n",
        "The [MedMNSIT](https://medmnist.com/) package (imported above) makes available several medical datasets available to access.\n",
        "\n",
        "You can change the `data_flag` variable (dataset names are all lower case letters) to the dataset of your choice (Take care to note the parameters e.g. number of input channels below that will affect your model)\n",
        "\n"
      ],
      "metadata": {
        "id": "S65n7FefE5T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "\n",
        "data_flag = 'pathmnist'\n",
        "download = True\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "print('Type of Machine Learning Task = ',task)\n",
        "print('Number of Input Data Channels = ',n_channels)\n",
        "print('Number of Classes = ',n_classes)\n",
        "print('The batch size for this dataset will be = ',BATCH_SIZE)\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ],
      "metadata": {
        "id": "caFqq_2T5uaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bfbe8af-2077-4a05-b369-d9c08306403b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of Machine Learning Task =  multi-class\n",
            "Number of Input Data Channels =  3\n",
            "Number of Classes =  9\n",
            "The batch size for this dataset will be =  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforms:\n",
        "You can update the functions below with appropriate transforms for your particular use case.\n",
        "\n",
        "* As well as being suitable for data augmention for image classification, the transforms.v2 package of torchvision extends transforms for object detection and segmentation tasks. An illustration of the transforms is shown [here](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py).\n",
        "* Normalisation based on ImageNet parameters is included already. This should be used with all models pre-trained on ImageNet  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yEcCkv14yJzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding Data augmentation: RamdomHorizontal Flip and Random Rotation\n",
        "train_transforms = v2.Compose([\n",
        "    v2.RandomHorizontalFlip(),\n",
        "    v2.RandomRotation(15),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    ToTensor(),\n",
        "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "test_transforms = v2.Compose([\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    ToTensor(),\n",
        "    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "lj7Ag9F9hBqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup your Medmnist dataset.\n",
        "* The convolution part of pre-trained networks (such as resnet) are compatable with any size input image. However they were trained on 224 x 224 size images, with early layers finding small scale features and deeper layers finding large scale features.\n",
        "* For this transfer learning application to medical data, the gap between the original ImageNet domain and the medical images is wide. Therefore the size/scale of the input images is less important, however in general we would expect better performance with the larger input images (as they contain more features at different scales).\n",
        "* You can add the `size=224` parameter to the dataset object calls, to load full size images. Only do this once you are confident in your training methodology (or if the dataset is small), as training with full size images will take longer.\n"
      ],
      "metadata": {
        "id": "daS_Z6OayI3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "train_dataset = DataClass(split='train', transform=train_transforms, download=download, size=224, mmap_mode='r')\n",
        "test_dataset = DataClass(split='test', transform=test_transforms, download=download, size=224, mmap_mode='r')\n",
        "val_dataset = DataClass(split='val', transform=test_transforms, download=download, size=224, mmap_mode='r')\n"
      ],
      "metadata": {
        "id": "CGcCuvXjE7TI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2bf644d-be81-40f3-f871-14f5b5c9a95e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 486M/12.6G [02:48<2:00:13, 1.68MB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Dataset` is passed as an argument to `DataLoader`. This wraps an\n",
        "iterable over the dataset, and supports automatic batching, sampling,\n",
        "shuffling and multiprocess data loading."
      ],
      "metadata": {
        "id": "LH_kTIFYIoTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "bX-snmo0Iv3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot some example augmented images"
      ],
      "metadata": {
        "id": "Nr4srFL0ZDU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output next batch from dataloader\n",
        "dataiter = iter(train_dataloader)\n",
        "image_batch, labels_batch = next(dataiter)\n",
        "\n",
        "# Use matplotlib to plot a sample of images\n",
        "\n",
        "i=0\n",
        "n_plots = 12 # number of plots\n",
        "f, axarr = plt.subplots(1,n_plots,figsize=(20,10))\n",
        "\n",
        "for image in image_batch[0:n_plots,:,:,:]:\n",
        "  disp_image =  torch.permute(image,(2,1,0)).numpy() # return image to cpu for display and permute to channels last\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  disp_image = std * disp_image + mean\n",
        "  disp_image = np.clip(disp_image, 0, 1)\n",
        "  axarr[i].imshow(disp_image[:,:,:])\n",
        "  axarr[i].axis(\"off\")\n",
        "  axarr[i].set_title(labels_batch[i].numpy(),fontsize='small')\n",
        "  i = i+1\n"
      ],
      "metadata": {
        "id": "Ofqjq8b4ZIJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Define Transfer Learning model\n",
        "Pytorch has an inbuilt [models package](https://pytorch.org/vision/stable/models.html) that allows loading of popular models with pre-trained weights.\n",
        "\n",
        "* We want to add an additional classifier stage (to the output of the network). How to setup the [model is detailed here](https://discuss.pytorch.org/t/load-only-a-part-of-the-network-with-pretrained-weights/88397/2).\n",
        "* This additional classifier may just be a single layer or a cascade of fully connected layers with dropout.\n",
        "* Note that the number of parameters in the convolutional part of the model will be same no what the input size is set to. However the output feature map size will vary with input image size (small for small image, large for large image). This means the number of parameters in the additional classifier will change depending on input image size.\n"
      ],
      "metadata": {
        "id": "TIIjqCm9IbxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "yZ1W5n704brZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Optimisation & Training Loop\n",
        "\n",
        "Define a training loop that prints the loss from the training and validation set at least every epoch.\n",
        "* You may choose to make the validation loss calculations more frequent so you can ensure training is progressing satisfactorily (especially with larger datasets).\n",
        "\n",
        "* You can use tensorboard to visualise the [loss curves](https://pytorch.org/docs/stable/tensorboard.html))\n",
        "\n",
        "* For multi-class classification classification problem we will use the  [CrossEntropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html). In this pytorch function, the input is the un-normalised logit value.\n",
        "\n",
        "* You may  have to use [torch.squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) to reduce the dimensionality of the label tensor before passing it to the loss function (this due to how the dataset is configured and the loss will only accept 0D or 1D inputs)"
      ],
      "metadata": {
        "id": "Q3_9Lw9zOtNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Optimisation Setup ##\n",
        "\n",
        "# Load pre-trained ResNet18\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Replace the final fully connected layer with 9 outputs (PathMNIST has 9 classes)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(model.fc.in_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(512, 9)   # 9 classes\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "F8M2JtkKg7xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training & Validation Loop##\n",
        "def train_loop(dataloader, model, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y = torch.squeeze(y)  # ensure labels are [N]\n",
        "\n",
        "        # Forward pass\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Accuracy\n",
        "        _, predicted = torch.max(pred, 1)\n",
        "        correct += (predicted == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = correct / total\n",
        "    print(f\"Train Loss: {avg_loss:.4f}, Train Acc: {acc:.4f}\")\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def val_loop(dataloader, model, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y = torch.squeeze(y)\n",
        "\n",
        "            pred = model(X)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Accuracy\n",
        "            _, predicted = torch.max(pred, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = correct / total\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Validation Acc: {acc:.4f}\")\n",
        "    return avg_loss, acc\n"
      ],
      "metadata": {
        "id": "inBKm1HkFi0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training Execution ##\n",
        "# Lists to store metrics\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, optimizer, device)\n",
        "    val_loss, val_acc = val_loop(val_dataloader, model, device)\n",
        "\n",
        "    # Save metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n"
      ],
      "metadata": {
        "id": "fpdmxfXFDDFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, epochs+1), train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(range(1, epochs+1), val_losses, 'r-', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1, epochs+1), train_accs, 'b-', label='Training Accuracy')\n",
        "plt.plot(range(1, epochs+1), val_accs, 'r-', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yLaE6nKwamOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialise Tensorboard (use of tensorboard in colab notebooks is [detailed here](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb)). A [Pytorch tutorial](https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html) shows how to setup the training and validation loop with pytorch.\n"
      ],
      "metadata": {
        "id": "r21Zjf3wx4Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs')"
      ],
      "metadata": {
        "id": "fd9d6fJIx7w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Evaluate Model\n",
        "You will need to setup an evaluation loop for the model to assess it's performance on the test dataset.\n",
        "\n",
        "You may also obtain a classification report after final evaluation of the test dataset with the model using the code below. A confusion matrix can also be obtained and we will plot a few example images.\n",
        "\n"
      ],
      "metadata": {
        "id": "T3KEd881eG-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://scikit-learn.org/0.16/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "from sklearn.metrics import confusion_matrix\n",
        "## Collect predictions and true labels from your validation set ## Rafa Code\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X, y in val_dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y = torch.squeeze(y)  # shape [N]\n",
        "        outputs = model(X)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(y.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Define class names (PathMNIST has 9 classes)\n",
        "# classes = list(INFO['pathmnist']['label'].values())\n",
        "# Keep numerical label since it harder to see the confusion matrix with long label names\n",
        "classes = [str(i) for i in range(9)]\n",
        "\n",
        "## End of Rafa code\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "plot_confusion_matrix(cm, classes) # Adding classes\n",
        "\n",
        "## Printing the labels descriptions to understand Confusion Matrix\n",
        "from medmnist import INFO\n",
        "\n",
        "# Get label mapping for PathMNIST\n",
        "pathmnist_labels = INFO['pathmnist']['label']\n",
        "\n",
        "# Print the label dictionary\n",
        "for idx, label in pathmnist_labels.items():\n",
        "    print(f\"{idx}: {label}\")"
      ],
      "metadata": {
        "id": "4jbe4FkwkEjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# returning the label name for the Classification Report\n",
        "classes = list(INFO['pathmnist']['label'].values())\n",
        "print(classification_report(y_true, y_pred, target_names=classes))"
      ],
      "metadata": {
        "id": "RN2167c_kKRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Comments about your Approach"
      ],
      "metadata": {
        "id": "zkZQ0yQS4JTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4tLRAvtuDS97"
      }
    }
  ]
}